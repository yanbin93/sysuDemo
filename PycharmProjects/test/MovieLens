from pyspark import SparkContext
import warnings
import matplotlib.pyplot as plt
import pylab as pl
import numpy as np
warnings.filterwarnings('ignore')
sc = SparkContext()
user_data = sc.textFile("file:///home/yanbin/data/ml-100k/u.user")
#print user_data.first()
user_fields = user_data.map(lambda line:line.split('|'))
num_users = user_fields.map(lambda fields:fields[0]).count()
num_genders = user_fields.map(lambda genders:genders[2]).distinct().count()
num_occupations = user_fields.map(lambda occupations:occupations[3]).distinct().count()
num_zipcodes =user_fields.map(lambda zipcodes:zipcodes[4]).distinct().count()
#print "Users:%d,genders:%d,occupations:%d,ZIP codes:%d" %(num_users,num_genders,num_occupations,num_zipcodes)
ages = user_fields.map(lambda ages:int(ages[1])).collect()

ax1 = plt.subplot(2,2,1)
ax2 = plt.subplot(2,2,2)
ax3 = plt.subplot(2,2,3)
ax1.hist(ages,bins=20,color='lightblue',normed=True)

count_occupations = user_fields.map(lambda occupations:(occupations[3],1)).reduceByKey(lambda x,y:x+y).collect()
x_axis1 = np.array([c[0] for c in count_occupations])
y_axis1 = np.array([c[1] for c in count_occupations])
x_axis =x_axis1[np.argsort(y_axis1)]
y_axis =y_axis1[np.argsort(y_axis1)]


pos = np.arange(len(x_axis))
width =1.0
ax2.set_xticks(pos+(width)/2)
ax2.set_xticklabels(x_axis)
ax2.bar(pos,y_axis,1.0,color='lightblue')
fig = plt.gcf()
fig.set_size_inches(30,30)
plt.show()